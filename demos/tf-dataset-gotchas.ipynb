{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook illustrates some examples of Tensorflow datasets, including tips and gotchas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics to make sure we understand how TF2 datasets work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some super-simple datasets of integers\n",
    "inc_dataset = tf.data.Dataset.range(100)\n",
    "dec_dataset = tf.data.Dataset.range(0, -100, -1)\n",
    "dataset = tf.data.Dataset.zip((inc_dataset, dec_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# Dump the first items in a dataset\n",
    "for elem in inc_dataset.take(3):\n",
    "  print(elem.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the batch size\n",
    "batched_dataset = inc_dataset.batch(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "[8, 9, 10, 11, 12, 13, 14, 15]\n",
      "[16, 17, 18, 19, 20, 21, 22, 23]\n",
      "[24, 25, 26, 27, 28, 29, 30, 31]\n",
      "[32, 33, 34, 35, 36, 37, 38, 39]\n",
      "[40, 41, 42, 43, 44, 45, 46, 47]\n",
      "[48, 49, 50, 51, 52, 53, 54, 55]\n",
      "[56, 57, 58, 59, 60, 61, 62, 63]\n",
      "[64, 65, 66, 67, 68, 69, 70, 71]\n",
      "[72, 73, 74, 75, 76, 77, 78, 79]\n",
      "[80, 81, 82, 83, 84, 85, 86, 87]\n",
      "[88, 89, 90, 91, 92, 93, 94, 95]\n",
      "[96, 97, 98, 99]\n"
     ]
    }
   ],
   "source": [
    "# Now ask for 15 of those batches.  Note that because there were only 100 elements, we only get 10 batches.\n",
    "for batch in batched_dataset.take(15):\n",
    "  print([arr.numpy() for arr in batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "[8, 9, 10, 11, 12, 13, 14, 15]\n",
      "[16, 17, 18, 19, 20, 21, 22, 23]\n",
      "[24, 25, 26, 27, 28, 29, 30, 31]\n",
      "[32, 33, 34, 35, 36, 37, 38, 39]\n",
      "[40, 41, 42, 43, 44, 45, 46, 47]\n",
      "[48, 49, 50, 51, 52, 53, 54, 55]\n",
      "[56, 57, 58, 59, 60, 61, 62, 63]\n",
      "[64, 65, 66, 67, 68, 69, 70, 71]\n",
      "[72, 73, 74, 75, 76, 77, 78, 79]\n",
      "[80, 81, 82, 83, 84, 85, 86, 87]\n",
      "[88, 89, 90, 91, 92, 93, 94, 95]\n",
      "[96, 97, 98, 99]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "[8, 9, 10, 11, 12, 13, 14, 15]\n"
     ]
    }
   ],
   "source": [
    "# Now try the same thing with the .repeat method.  Notice that now we get 15 batches, and it repeats.\n",
    "for batch in batched_dataset.repeat().take(15):\n",
    "  print([arr.numpy() for arr in batch])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to understand how non-tensorflow operations work and when they execute.  \n",
    "Consider two versions of an \"augment\" function that simply adds a random number to each element\n",
    "of a dataset containing integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use numpy to generate the random number\n",
    "def augment(x):\n",
    "    result = x + np.round(np.random.uniform(0, 3))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use TF to generate the random number.\n",
    "def tf_augment(x):\n",
    "    # We pass in [] as the first argument to uniform() to indicate that we want a scalar back instead of a tensor.\n",
    "    result = x + tf.round(tf.random.uniform([], 0, 3, dtype=tf.dtypes.int64))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(6, shape=(), dtype=int64)\n",
      "tf.Tensor(7, shape=(), dtype=int64)\n",
      "tf.Tensor(6, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# Confirm that we get a random number each time we call tf_augment.\n",
    "for i in range(3):\n",
    "    print(tf_augment(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's insert these augment functions into a dataset pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "\n",
      "Iteration 1\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "\n",
      "Iteration 2\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "# When you use the augment() which uses numpy to generate the random offset, you get \n",
    "# the same offset applied to all elements drawn from the dataset.  On each iteration you get \n",
    "# a different offset, but the same offset is applied within an iteration.  \n",
    "# THIS PROBABLY ISN'T WHAT YOU WANTED.\n",
    "for i in range(3):\n",
    "    print(\"\\nIteration {}\".format(i))\n",
    "    for elt in inc_dataset.map(augment).take(15):\n",
    "      print(elt.numpy())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 0\n",
      "1\n",
      "1\n",
      "2\n",
      "4\n",
      "6\n",
      "6\n",
      "6\n",
      "8\n",
      "10\n",
      "9\n",
      "10\n",
      "13\n",
      "13\n",
      "15\n",
      "16\n",
      "\n",
      "Iteration 1\n",
      "1\n",
      "1\n",
      "3\n",
      "5\n",
      "5\n",
      "5\n",
      "7\n",
      "7\n",
      "9\n",
      "9\n",
      "11\n",
      "11\n",
      "14\n",
      "15\n",
      "14\n",
      "\n",
      "Iteration 2\n",
      "2\n",
      "1\n",
      "3\n",
      "4\n",
      "6\n",
      "6\n",
      "7\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "13\n",
      "14\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "# On the other hand, when you use the augment that uses TF to generate the random numbers,\n",
    "# different random offsets are used for different elements in the dataset.  \n",
    "# THIS IS PROBABLY WHAT YOU WANT.\n",
    "for i in range(3):\n",
    "    print(\"\\nIteration {}\".format(i))\n",
    "    for elt in inc_dataset.map(tf_augment).take(15):\n",
    "      print(elt.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: A tensorflow pipeline operates on tensors.  Anything else is evaluated once and held fixed.  \n",
    "I.e. it's inserted in the graph as a tf.constant.\n",
    "\n",
    "So the numpy.random.uniform() gets called exactly once, instead of on each element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "2\n",
      "5\n",
      "4\n",
      "6\n",
      "8\n",
      "9\n",
      "8\n",
      "9\n",
      "11\n",
      "13\n",
      "12\n",
      "15\n",
      "15\n",
      "16\n",
      "18\n",
      "19\n",
      "18\n",
      "20\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "24\n",
      "26\n",
      "26\n",
      "29\n",
      "28\n",
      "31\n",
      "30\n",
      "32\n",
      "34\n",
      "34\n",
      "34\n",
      "36\n",
      "38\n",
      "39\n",
      "38\n",
      "39\n",
      "40\n",
      "42\n",
      "42\n",
      "43\n",
      "46\n",
      "47\n",
      "48\n",
      "48\n",
      "48\n",
      "50\n",
      "52\n",
      "51\n",
      "54\n",
      "53\n",
      "56\n",
      "56\n",
      "57\n",
      "57\n",
      "58\n",
      "60\n",
      "62\n",
      "61\n",
      "63\n",
      "63\n",
      "66\n",
      "66\n",
      "67\n",
      "69\n",
      "70\n",
      "70\n",
      "70\n",
      "71\n",
      "72\n",
      "74\n",
      "76\n",
      "75\n",
      "78\n",
      "77\n",
      "80\n",
      "81\n",
      "80\n",
      "83\n",
      "83\n",
      "83\n",
      "85\n",
      "87\n",
      "88\n",
      "88\n",
      "90\n",
      "90\n",
      "91\n",
      "92\n",
      "94\n",
      "95\n",
      "96\n",
      "96\n",
      "96\n",
      "98\n",
      "99\n",
      "100\n",
      "0\n",
      "2\n",
      "3\n",
      "4\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "# Now let's test what happens if we oversample from the dataset.  \n",
    "# Here we'll draw 105 elements from the 100-element dataset.  \n",
    "# (Note that to do this we have to insert a repeat() call.)\n",
    "# Observe that different random offsets are applied to the last 5 elements than\n",
    "# the first 5 elements, even though their base value in inc_dataset is the same.\n",
    "# This tells us that the random values do not repeat.  Which is a good thing.\n",
    "\n",
    "for elt in inc_dataset.map(tf_augment).repeat().take(105):\n",
    "    print(elt.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 3, 3, 4, 5, 7, 8]\n",
      "[9, 11, 12, 13, 12, 13, 16, 16]\n",
      "[17, 18, 18, 19, 21, 22, 22, 24]\n",
      "[25, 26, 26, 28, 30, 30, 30, 31]\n",
      "[32, 35, 34, 35, 37, 38, 39, 40]\n",
      "[41, 43, 42, 44, 44, 45, 47, 47]\n",
      "[50, 51, 51, 53, 53, 55, 55, 57]\n",
      "[58, 57, 60, 60, 62, 61, 63, 65]\n",
      "[66, 65, 66, 69, 70, 70, 70, 73]\n",
      "[72, 73, 74, 76, 76, 79, 78, 81]\n",
      "[80, 83, 84, 83, 86, 86, 88, 87]\n",
      "[88, 89, 91, 93, 92, 94, 94, 95]\n",
      "[97, 97, 98, 99]\n",
      "[2, 2, 4, 5, 6, 6, 6, 7]\n",
      "[9, 9, 10, 11, 13, 13, 14, 16]\n"
     ]
    }
   ],
   "source": [
    "# Here we see that the same holds true even when you put batches into the picture.  \n",
    "# When batches repeat, they do so with different random offsets applied to them.\n",
    "# Again, this is a good thing.\n",
    "for batch in inc_dataset.map(tf_augment).batch(8).repeat().take(15):\n",
    "  print([arr.numpy() for arr in batch])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 5, 12, 17, 22, 13, 11, 23, 15]), array([ -5, -12, -17, -22, -13, -11, -23, -15])]\n",
      "[array([21, 20,  2, 10, 29,  4,  9,  0]), array([-21, -20,  -2, -10, -29,  -4,  -9,   0])]\n",
      "[array([32, 27, 30,  1, 36, 35, 33, 24]), array([-32, -27, -30,  -1, -36, -35, -33, -24])]\n",
      "[array([31,  6,  7,  8, 43, 44, 42, 19]), array([-31,  -6,  -7,  -8, -43, -44, -42, -19])]\n",
      "[array([ 3, 28, 48, 53, 18, 41, 26, 16]), array([ -3, -28, -48, -53, -18, -41, -26, -16])]\n",
      "[array([38, 14, 50, 62, 60, 47, 34, 64]), array([-38, -14, -50, -62, -60, -47, -34, -64])]\n",
      "[array([56, 55, 57, 58, 71, 37, 49, 65]), array([-56, -55, -57, -58, -71, -37, -49, -65])]\n",
      "[array([25, 66, 69, 63, 78, 70, 54, 82]), array([-25, -66, -69, -63, -78, -70, -54, -82])]\n",
      "[array([40, 73, 80, 85, 77, 67, 83, 75]), array([-40, -73, -80, -85, -77, -67, -83, -75])]\n",
      "[array([74, 90, 79, 92, 86, 96, 84, 46]), array([-74, -90, -79, -92, -86, -96, -84, -46])]\n",
      "[array([95, 98, 89, 91, 51, 45, 59, 94]), array([-95, -98, -89, -91, -51, -45, -59, -94])]\n",
      "[array([61, 68, 87, 72, 99, 52, 81, 76]), array([-61, -68, -87, -72, -99, -52, -81, -76])]\n",
      "[array([39, 88, 93, 97]), array([-39, -88, -93, -97])]\n",
      "[array([ 4, 16, 13,  8, 19, 18, 22, 21]), array([ -4, -16, -13,  -8, -19, -18, -22, -21])]\n",
      "[array([20, 15, 25, 23, 14, 17, 12, 31]), array([-20, -15, -25, -23, -14, -17, -12, -31])]\n"
     ]
    }
   ],
   "source": [
    "# Here is how you can set up shuffling so that each each batch is unique.  \n",
    "# NOTE THAT THIS IS NOT HOW I NORMALLY THINK OF BATCHES.  Also note that this will be\n",
    "# quite slow in practice if you reshuffle with a buffer that is the size of 100,000 elements.\n",
    "for batch in dataset.shuffle(20, seed=5).batch(8).repeat().take(15):\n",
    "  print([arr.numpy() for arr in batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([64, 65, 66, 67, 68, 69, 70, 71]), array([-64, -65, -66, -67, -68, -69, -70, -71])]\n",
      "[array([96, 97, 98, 99]), array([-96, -97, -98, -99])]\n",
      "[array([88, 89, 90, 91, 92, 93, 94, 95]), array([-88, -89, -90, -91, -92, -93, -94, -95])]\n",
      "[array([ 8,  9, 10, 11, 12, 13, 14, 15]), array([ -8,  -9, -10, -11, -12, -13, -14, -15])]\n",
      "[array([80, 81, 82, 83, 84, 85, 86, 87]), array([-80, -81, -82, -83, -84, -85, -86, -87])]\n",
      "[array([56, 57, 58, 59, 60, 61, 62, 63]), array([-56, -57, -58, -59, -60, -61, -62, -63])]\n",
      "[array([24, 25, 26, 27, 28, 29, 30, 31]), array([-24, -25, -26, -27, -28, -29, -30, -31])]\n",
      "[array([40, 41, 42, 43, 44, 45, 46, 47]), array([-40, -41, -42, -43, -44, -45, -46, -47])]\n",
      "[array([16, 17, 18, 19, 20, 21, 22, 23]), array([-16, -17, -18, -19, -20, -21, -22, -23])]\n",
      "[array([48, 49, 50, 51, 52, 53, 54, 55]), array([-48, -49, -50, -51, -52, -53, -54, -55])]\n",
      "[array([32, 33, 34, 35, 36, 37, 38, 39]), array([-32, -33, -34, -35, -36, -37, -38, -39])]\n",
      "[array([0, 1, 2, 3, 4, 5, 6, 7]), array([ 0, -1, -2, -3, -4, -5, -6, -7])]\n",
      "[array([72, 73, 74, 75, 76, 77, 78, 79]), array([-72, -73, -74, -75, -76, -77, -78, -79])]\n",
      "[array([88, 89, 90, 91, 92, 93, 94, 95]), array([-88, -89, -90, -91, -92, -93, -94, -95])]\n",
      "[array([32, 33, 34, 35, 36, 37, 38, 39]), array([-32, -33, -34, -35, -36, -37, -38, -39])]\n"
     ]
    }
   ],
   "source": [
    "# Here is how you set up shuffling so that each batch is the same but the \n",
    "# order of the batches is random. THIS IS HOW I USUALLY THINK OF THINGS.\n",
    "for batch in dataset.batch(8).shuffle(20,seed=5).repeat().take(15):\n",
    "  print([arr.numpy() for arr in batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([11, 20,  6, 22, 16, 19, 15, 24]), array([-11, -20,  -6, -22, -16, -19, -15, -24])]\n",
      "[array([ 7, 28,  5, 27,  1, 14, 10, 17]), array([ -7, -28,  -5, -27,  -1, -14, -10, -17])]\n",
      "[array([25, 18, 32,  9,  4, 40,  0, 39]), array([-25, -18, -32,  -9,  -4, -40,   0, -39])]\n",
      "[array([ 3, 31, 26, 37, 34, 29, 35, 13]), array([ -3, -31, -26, -37, -34, -29, -35, -13])]\n",
      "[array([33, 45, 51,  8, 48, 38, 30,  2]), array([-33, -45, -51,  -8, -48, -38, -30,  -2])]\n",
      "[array([46, 55, 41, 47, 63, 50, 42, 57]), array([-46, -55, -41, -47, -63, -50, -42, -57])]\n",
      "[array([61, 52, 36, 21, 64, 70, 12, 72]), array([-61, -52, -36, -21, -64, -70, -12, -72])]\n",
      "[array([23, 66, 59, 56, 44, 79, 77, 78]), array([-23, -66, -59, -56, -44, -79, -77, -78])]\n",
      "[array([76, 49, 85, 65, 81, 74, 67, 86]), array([-76, -49, -85, -65, -81, -74, -67, -86])]\n",
      "[array([75, 69, 93, 73, 95, 62, 90, 94]), array([-75, -69, -93, -73, -95, -62, -90, -94])]\n",
      "[array([83, 88, 53, 92, 82, 43, 98, 71]), array([-83, -88, -53, -92, -82, -43, -98, -71])]\n",
      "[array([80, 87, 89, 99, 60, 58, 54, 97]), array([-80, -87, -89, -99, -60, -58, -54, -97])]\n",
      "[array([96, 91, 84, 68]), array([-96, -91, -84, -68])]\n",
      "[array([11, 20,  6, 22, 16, 19, 15, 24]), array([-11, -20,  -6, -22, -16, -19, -15, -24])]\n",
      "[array([ 7, 28,  5, 27,  1, 14, 10, 17]), array([ -7, -28,  -5, -27,  -1, -14, -10, -17])]\n"
     ]
    }
   ],
   "source": [
    "# If you want to shuffle before you create batches, e.g. to create train/test splits, you \n",
    "# want to be sure that those batches don't get redone after each epoch.  If they do,\n",
    "# then elements that were in training batches on epoch 1 may end up in validation batches on epoch 2.\n",
    "# To avoid this, use the reshuffle_each_iteration flag\n",
    "for batch in dataset.shuffle(20, seed=5, reshuffle_each_iteration=False).batch(8).repeat().take(15):\n",
    "  print([arr.numpy() for arr in batch])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A crude way to do train/val/test splits\n",
    "\n",
    "I don't really recommend this except if you're seriously pressed for time.  \n",
    "A far better way is to use either scikit-learn's `test_train_split` or \n",
    "`tf.data.experimental.sample_from_datasets` or `tf.data.experimental.rejection_resample`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = 75\n",
    "num_val = 10\n",
    "num_test = 15\n",
    "train_ds = dataset.take(num_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "nontrain_ds = dataset.skip(num_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0]\n",
      "[1, -1]\n",
      "[2, -2]\n",
      "[3, -3]\n",
      "[4, -4]\n",
      "[5, -5]\n",
      "[6, -6]\n",
      "[7, -7]\n",
      "[8, -8]\n",
      "[9, -9]\n",
      "[10, -10]\n",
      "[11, -11]\n",
      "[12, -12]\n",
      "[13, -13]\n",
      "[14, -14]\n",
      "[15, -15]\n",
      "[16, -16]\n",
      "[17, -17]\n",
      "[18, -18]\n",
      "[19, -19]\n",
      "[20, -20]\n",
      "[21, -21]\n",
      "[22, -22]\n",
      "[23, -23]\n",
      "[24, -24]\n",
      "[25, -25]\n",
      "[26, -26]\n",
      "[27, -27]\n",
      "[28, -28]\n",
      "[29, -29]\n",
      "[30, -30]\n",
      "[31, -31]\n",
      "[32, -32]\n",
      "[33, -33]\n",
      "[34, -34]\n",
      "[35, -35]\n",
      "[36, -36]\n",
      "[37, -37]\n",
      "[38, -38]\n",
      "[39, -39]\n",
      "[40, -40]\n",
      "[41, -41]\n",
      "[42, -42]\n",
      "[43, -43]\n",
      "[44, -44]\n",
      "[45, -45]\n",
      "[46, -46]\n",
      "[47, -47]\n",
      "[48, -48]\n",
      "[49, -49]\n",
      "[50, -50]\n",
      "[51, -51]\n",
      "[52, -52]\n",
      "[53, -53]\n",
      "[54, -54]\n",
      "[55, -55]\n",
      "[56, -56]\n",
      "[57, -57]\n",
      "[58, -58]\n",
      "[59, -59]\n",
      "[60, -60]\n",
      "[61, -61]\n",
      "[62, -62]\n",
      "[63, -63]\n",
      "[64, -64]\n",
      "[65, -65]\n",
      "[66, -66]\n",
      "[67, -67]\n",
      "[68, -68]\n",
      "[69, -69]\n",
      "[70, -70]\n",
      "[71, -71]\n",
      "[72, -72]\n",
      "[73, -73]\n",
      "[74, -74]\n"
     ]
    }
   ],
   "source": [
    "for elem in train_ds:\n",
    "    print([arr.numpy() for arr in elem])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[75, -75]\n",
      "[76, -76]\n",
      "[77, -77]\n",
      "[78, -78]\n",
      "[79, -79]\n",
      "[80, -80]\n",
      "[81, -81]\n",
      "[82, -82]\n",
      "[83, -83]\n",
      "[84, -84]\n"
     ]
    }
   ],
   "source": [
    "val_ds = nontrain_ds.take(num_val)\n",
    "for elem in val_ds:\n",
    "    print([arr.numpy() for arr in elem])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[85, -85]\n",
      "[86, -86]\n",
      "[87, -87]\n",
      "[88, -88]\n",
      "[89, -89]\n",
      "[90, -90]\n",
      "[91, -91]\n",
      "[92, -92]\n",
      "[93, -93]\n",
      "[94, -94]\n",
      "[95, -95]\n",
      "[96, -96]\n",
      "[97, -97]\n",
      "[98, -98]\n",
      "[99, -99]\n"
     ]
    }
   ],
   "source": [
    "test_ds = nontrain_ds.skip(num_val)\n",
    "for elem in test_ds:\n",
    "    print([arr.numpy() for arr in elem])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
